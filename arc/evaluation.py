#!/usr/bin/env python
"""
Functions for evaluating model performance on ARC tasks.
"""
import numpy as np
import logging
from typing import Dict, List, Any, Optional, Tuple
from arc.visualization import parse_grid_from_text

# Set up logging
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('arc.evaluation')

def intersection_over_union(y_true, y_pred, class_val):
    """
    Calculate Intersection over Union for a specific class value.
    
    Args:
        y_true: Ground truth grid (2D numpy array)
        y_pred: Predicted grid (2D numpy array)
        class_val: The class value to calculate IoU for
        
    Returns:
        IoU score for the specified class (float between 0 and 1)
    """
    # Create binary masks
    true_mask = (y_true == class_val)
    pred_mask = (y_pred == class_val)
    
    # Calculate intersection and union
    intersection = np.logical_and(true_mask, pred_mask).sum()
    union = np.logical_or(true_mask, pred_mask).sum()
    
    # Log details for debugging
    logger.debug(f"Class {class_val} - Intersection: {intersection}, Union: {union}")
    
    # Calculate IoU (handle division by zero)
    if union == 0:
        # If the class doesn't appear in either ground truth or prediction
        if intersection == 0:
            logger.debug(f"Class {class_val} absent in both ground truth and prediction")
            return 1.0  # Both agree this class is absent
        else:
            logger.warning(f"Unexpected case: intersection={intersection}, union=0 for class {class_val}")
            return 0.0  # This should not happen mathematically
    
    iou = intersection / union
    logger.debug(f"Class {class_val} IoU: {iou:.4f}")
    return float(iou)

def mean_iou(actual, predicted):
    """
    Calculate mean Intersection over Union (IoU) between actual and predicted grids.
    
    Args:
        actual: Ground truth grid (numpy array)
        predicted: Predicted grid (numpy array)
        
    Returns:
        Mean IoU score (0-1 scale, higher is better)
    """
    # Check for dimension mismatch and return 0 if dimensions don't match
    if actual.shape != predicted.shape:
        logger.warning(f"Shape mismatch: actual {actual.shape} vs predicted {predicted.shape}")
        return 0.0
    
    # Get unique values across both grids
    all_values = np.unique(np.concatenate((actual.flatten(), predicted.flatten())))
    logger.debug(f"Unique values in grids: {all_values}")
    
    # Calculate IoU for each unique value
    iou_scores = []
    for val in all_values:
        # Skip background (0s) for IoU calculation
        if val == 0:
            logger.debug(f"Skipping background class (0) for IoU calculation")
            continue
            
        # Create binary masks for this value
        mask_actual = (actual == val)
        mask_predicted = (predicted == val)
        
        # Calculate intersection and union
        intersection = np.logical_and(mask_actual, mask_predicted).sum()
        union = np.logical_or(mask_actual, mask_predicted).sum()
        
        # Calculate IoU for this value
        if union > 0:
            iou = intersection / union
            logger.debug(f"Class {val} - Intersection: {intersection}, Union: {union}, IoU: {iou:.4f}")
            iou_scores.append(iou)
        else:
            logger.debug(f"Class {val} - No elements in union")
    
    # Calculate mean IoU across all values
    if len(iou_scores) > 0:
        mean = np.mean(iou_scores)
        logger.info(f"Mean IoU: {mean:.4f} across {len(iou_scores)} classes")
        return mean
    else:
        logger.warning("No non-background classes matched. Returning IoU=0")
        return 0.0  # If no non-background values matched

def evaluate_solution(task, solution_text):
    """
    Evaluate a solution for an ARC task.
    
    Args:
        task: The ARC task
        solution_text: The solution text generated by the model
        
    Returns:
        Dict with evaluation metrics
    """
    try:
        logger.info(f"Evaluating solution for task {task.get('id', 'unknown')}")
        logger.debug(f"Solution text:\n{solution_text}")
        
        # Use the extract_grid_from_solution method from prompts module instead
        from arc.prompts import extract_grid_from_solution
        
        # Parse the predicted grid from the solution text using the more robust method
        predicted_grid = extract_grid_from_solution(solution_text)
        
        # Initialize metrics
        metrics = {
            'exact_match': False,
            'mean_iou': 0.0,
            'valid_solution': predicted_grid is not None
        }
        
        # If we have a parsed grid, add it to the metrics
        if predicted_grid is not None:
            logger.info(f"Successfully parsed grid with shape {predicted_grid.shape}")
            logger.debug(f"Parsed grid:\n{predicted_grid}")
            metrics['prediction'] = predicted_grid
            
            # Ground truth for comparison
            ground_truth = np.array(task['test'][0]['output'])
            logger.debug(f"Ground truth grid shape: {ground_truth.shape}")
            logger.debug(f"Ground truth grid:\n{ground_truth}")
            
            # Compute metrics
            evaluation_results = evaluate_prediction(ground_truth, predicted_grid)
            metrics.update(evaluation_results)
            logger.info(f"Evaluation results: exact_match={metrics['exact_match']}, mean_iou={metrics['mean_iou']:.4f}")
        else:
            # Handle failed grid extraction
            logger.warning("Failed to parse grid from solution text")
            metrics.update({
                'shape_match': False,
                'accuracy': 0.0,
                'extraction_failed': True
            })
        
        return metrics
    except Exception as e:
        # Log the error but return a valid dictionary
        logger.error(f"Error in evaluate_solution: {e}", exc_info=True)
        return {
            'exact_match': False,
            'mean_iou': 0.0,
            'valid_solution': False,
            'error': str(e)
        }

def evaluate_prediction(actual, predicted):
    """
    Evaluate prediction with multiple metrics.
    
    Args:
        actual: Ground truth grid (numpy array)
        predicted: Predicted grid (numpy array)
        
    Returns:
        Dictionary of metrics
    """
    # Check for dimension mismatch
    shape_match = (actual.shape == predicted.shape)
    
    if not shape_match:
        logger.warning(f"Shape mismatch: actual {actual.shape} vs predicted {predicted.shape}")
    
    # If shapes don't match, we can't do exact match
    exact_match = False
    accuracy = 0.0
    miou = 0.0
    
    # Only compute detailed metrics if shapes match
    if shape_match:
        # Calculate exact match
        exact_match = np.array_equal(actual, predicted)
        if exact_match:
            logger.info("EXACT MATCH! ðŸŽ‰ Prediction perfectly matches ground truth.")
        
        # Calculate cell-wise accuracy
        total_cells = actual.size
        correct_cells = (actual == predicted).sum()
        accuracy = correct_cells / total_cells
        logger.info(f"Cell-wise accuracy: {accuracy:.4f} ({correct_cells}/{total_cells} cells correct)")
        
        # Calculate mean IoU
        miou = mean_iou(actual, predicted)
        logger.info(f"Mean IoU: {miou:.4f}")
    
    return {
        'exact_match': exact_match,
        'shape_match': shape_match,
        'mean_iou': miou,
        'accuracy': accuracy
    }

def analyze_results(results, verbose=True):
    """
    Analyze the results of multiple inference attempts.
    
    Args:
        results: Dictionary of results from run_multiple_inferences
        verbose: Whether to print analysis
        
    Returns:
        Dictionary of analyzed data
    """
    metrics = results['metrics']
    task_id = results.get('task_id', 'unknown')
    
    logger.info(f"Analyzing results for task {task_id} with {len(metrics)} inference attempts")
    
    # Extract IoU scores and filter out None values
    iou_scores = [m.get('mean_iou', 0.0) for m in metrics if m is not None]
    
    # Log individual attempt scores
    for i, score in enumerate(iou_scores):
        logger.info(f"Attempt {i+1}: IoU = {score:.4f}")
    
    if not iou_scores:
        logger.warning("No valid IoU scores found.")
        return {
            'average_iou': 0.0,
            'max_iou': 0.0,
            'min_iou': 0.0,
            'above_average_indices': [],
            'best_attempt_index': None,
            'success_rate': 0.0
        }
    
    # Calculate statistics
    average_iou = np.mean(iou_scores)
    max_iou = np.max(iou_scores)
    min_iou = np.min(iou_scores)
    
    # Find attempts above average
    above_average_indices = [i for i, score in enumerate(iou_scores) if score > average_iou]
    
    # Find the best attempt
    best_attempt_index = np.argmax(iou_scores)
    
    # Calculate success rate (exact matches)
    exact_matches = sum(1 for m in metrics if m.get('exact_match', False))
    success_rate = exact_matches / len(metrics) if metrics else 0
    
    logger.info(f"Task {task_id} Results Summary:")
    logger.info(f"Average IoU Score: {average_iou:.4f}")
    logger.info(f"Max IoU Score: {max_iou:.4f} (Attempt #{best_attempt_index + 1})")
    logger.info(f"Min IoU Score: {min_iou:.4f}")
    logger.info(f"Success Rate (Exact Matches): {success_rate:.2%}")
    logger.info(f"Attempts Above Average: {len(above_average_indices)}/{len(iou_scores)}")
    
    if verbose:
        print("\nResults Analysis:")
        print(f"Average IoU Score: {average_iou:.4f}")
        print(f"Max IoU Score: {max_iou:.4f} (Attempt #{best_attempt_index + 1})")
        print(f"Min IoU Score: {min_iou:.4f}")
        print(f"Success Rate (Exact Matches): {success_rate:.2%}")
        print(f"Attempts Above Average: {len(above_average_indices)}/{len(iou_scores)}")
    
    return {
        'average_iou': average_iou,
        'max_iou': max_iou,
        'min_iou': min_iou,
        'above_average_indices': above_average_indices,
        'best_attempt_index': best_attempt_index,
        'success_rate': success_rate
    } 